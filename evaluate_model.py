import torch
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from transformers.generation.streamers import TextStreamer
import os

# ==============================================================================
# SETUP
# ==============================================================================
MODEL_PATH = "outputs/gemma-3n-quechua"
BASE_MODEL = "unsloth/gemma-3n-E4B-it"

# ==============================================================================
# CARGAR MODELO FINE-TUNEADO
# ==============================================================================
print("ğŸ”„ Cargando modelo fine-tuneado...")

# Verificar si existe el modelo fine-tuneado
if os.path.exists(MODEL_PATH):
    print(f"âœ… Encontrado modelo en: {MODEL_PATH}")
    try:
        # Cargar modelo con Unsloth
        model, tokenizer = FastModel.from_pretrained(
            model_name=MODEL_PATH,
            max_seq_length=2048,
            dtype=None,
            load_in_4bit=True,
        )
        print("âœ… Modelo cargado exitosamente con Unsloth")
    except Exception as e:
        print(f"âŒ Error cargando con Unsloth: {e}")
        print("ğŸ”„ Intentando cargar modelo base...")
        model, tokenizer = FastModel.from_pretrained(
            model_name=BASE_MODEL,
            max_seq_length=2048,
            dtype=None,
            load_in_4bit=True,
        )
        print("âš ï¸ Cargado modelo base (sin fine-tuning)")
else:
    print(f"âŒ No se encontrÃ³ modelo en: {MODEL_PATH}")
    print("ğŸ”„ Cargando modelo base...")
    model, tokenizer = FastModel.from_pretrained(
        model_name=BASE_MODEL,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )
    print("âš ï¸ Cargado modelo base (sin fine-tuning)")

# ==============================================================================
# CONFIGURAR CHAT TEMPLATE
# ==============================================================================
print("ğŸ”§ Configurando chat template...")
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3",
)


# ==============================================================================
# FUNCIÃ“N DE PRUEBA
# ==============================================================================
def test_model(prompt_text, max_tokens=128, show_stream=True):
    """
    FunciÃ³n para probar el modelo con un prompt dado.
    Args:
        prompt_text (str): Texto del prompt.
        max_tokens (int, optional): NÃºmero mÃ¡ximo de tokens a generar. Defaults to 128.
        show_stream (bool, optional): Mostrar salida en streaming. Defaults to True.
    """
    # Crear mensaje en formato correcto
    messages = [{"role": "user", "content": [{"type": "text", "text": prompt_text}]}]

    # Preparar input
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt",
        tokenize=True,
        return_dict=True,
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    print(f"ğŸ§ª Prompt: {prompt_text}")
    print("ğŸ¤– Respuesta: ", end="")

    if show_stream:
        # Generar con streaming
        streamer = TextStreamer(tokenizer, skip_prompt=True)
        with torch.no_grad():
            _ = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=1.0,
                top_p=0.95,
                top_k=64,
                streamer=streamer,
                do_sample=True,
            )
    else:
        # Generar sin streaming
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=1.0,
                top_p=0.95,
                top_k=64,
                do_sample=True,
            )

        # Decodificar respuesta
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Extraer solo la respuesta del modelo
        if "<start_of_turn>model" in response:
            response = response.split("<start_of_turn>model")[-1].strip()

        print(response)

    print("-" * 60)


# ==============================================================================
# EXMPLOS DE PRUEBA
# ==============================================================================
test_examples = [
    # TraducciÃ³n espaÃ±ol -> quechua
    "Traduce al quechua: Hola, Â¿cÃ³mo estÃ¡s?",
    "Traduce al quechua: Gracias por tu ayuda.",
    "Traduce al quechua: Me gusta la mÃºsica.",
    "Traduce al quechua: Â¿DÃ³nde estÃ¡ la escuela?",
    "Traduce al quechua: Amarillo",
    "Traduce al quechua: Yo cosecho maiz para la chala",
    "Traduce al quechua: Â¿En quÃ© circunstancias le pegÃ³ su esposo?",
    "Traduce al quechua: Cuando comenzamos a discutir.",
    "Traduce al quechua: SeÃ±or, Â¿QuÃ© dÃ­as abre esta oficina?",
    "Traduce al quechua: La oficina atiende lunes, miÃ©rcoles y viernes.",
    "Traduce al quechua: La oficina atiende martes y jueves.",
    "Traduce al quechua: Regrese maÃ±ana en la maÃ±ana.",
    "Traduce al quechua: Regrese maÃ±ana en la tarde.",
    "Traduce al quechua: Regrese pasado maÃ±ana.",
    "Traduce al quechua: Regrese el siguiente lunes.",
    "Traduce al quechua: Hoy no hay atenciÃ³n, es feriado.",
    "Traduce al quechua: Hoy no hay atenciÃ³n, los trabajadores estÃ¡n de huelga.",
    # TraducciÃ³n quechua -> espaÃ±ol
    "Traduce al espaÃ±ol: Allin pÂ´unchaw wiraqucha",
    "Traduce al espaÃ±ol: Allin pÂ´unchaw doctor",
    "Traduce al espaÃ±ol: Allin pÂ´unchay taytay",
    "Traduce al espaÃ±ol: Allin pâ€™unchaw kachun runaymasiykuna",
    "Traduce al espaÃ±ol: Wiraqucha Â¿Imay horastan llamkâ€™ayta qallarinku?",
    "Traduce al espaÃ±ol: Kay oficina pusaq horastakichakun",
    "Traduce al espaÃ±ol: Chunca iskay pacha kimsachunkayuqta kutimunki",
    "Traduce al espaÃ±ol: Kay wasiqa pichqa pachatan wisqâ€™akunqa",
    "Traduce al espaÃ±ol: Allinllachu?",
    "Traduce al espaÃ±ol: AÃ±ay kusay.",
    # Preguntas sobre quechua
    "Â¿CÃ³mo se dice 'agua' en quechua?",
    "Â¿CuÃ¡les son los nÃºmeros del 1 al 5 en quechua?",
    # ConversaciÃ³n en quechua
    "Responde en quechua: Â¿Castellano simita yachankichu, icha rimay tâ€™ikraqtachu munawaq?",
]

# ==============================================================================
# RUN TESTS
# ==============================================================================
if __name__ == "__main__":
    print("ğŸš€ Iniciando pruebas del modelo...")
    print("=" * 60)

    # Mostrar informaciÃ³n del dispositivo
    if torch.cuda.is_available():
        print(f"ğŸ”¥ GPU disponible: {torch.cuda.get_device_name()}")
        print(
            f"ğŸ’¾ VRAM libre: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB"
        )
    else:
        print("ğŸ’» Usando CPU")

    print("=" * 60)

    # Ejecutar todas las pruebas
    for i, example in enumerate(test_examples, 1):
        print(f"\nğŸ“ Prueba {i}/{len(test_examples)}:")
        try:
            test_model(example, max_tokens=100, show_stream=True)
        except Exception as e:
            print(f"âŒ Error en prueba {i}: {e}")
            continue

    print("\nâœ… Pruebas completadas!")

    # Prueba interactiva opcional
    print("\n" + "=" * 60)
    print("ğŸ¯ Â¿Quieres hacer una prueba interactiva? (s/n)")
    if input().lower().startswith("s"):
        while True:
            user_input = input("\nğŸ“ Tu pregunta (o 'salir' para terminar): ")
            if user_input.lower() in ["salir", "exit", "quit"]:
                break
            try:
                test_model(user_input, max_tokens=150, show_stream=True)
            except Exception as e:
                print(f"âŒ Error: {e}")

    print("ğŸ‘‹ Â¡Se ha terminado la prueba!")
